%\documentclass[11pt,leqno]{book}
%\usepackage[spanish,activeacute]{babel}
%\usepackage[utf8]{inputenc}
%\usepackage{enumerate}
%
%\begin{document}


\chapter{Desarrollo matemático}
\label{ch:matematicas}

En este capítulo vamos a ver en primer lugar una herramienta muy importante para la reducción de dimensiones cuando se tienen un gran conjunto de datos que están midiendo una misma información, lo que hace que ese conjunto de datos tenga, normalmente, la propiedad de que estén muy correlados entre sí. Esta herramienta es el análisis de componentes principales, y permite obtener unos nuevos ejes coordenados combinaciones lineales de los datos observados inicialmente pero incorrelados entre sí, de forma que al poner los datos iniciales en función de estos nuevos ejes obtenemos otra representación de los mismos datos. Además, podemos elegir qué cantidad de información estamos dispuestos a perder eligiendo cuántas componentes deseamos utilizar para obtener los datos en los nuevos ejes, ya que a más componentes menos información se pierde, pero menos reducción de datos a almacenar hay también. Por tanto, se trata de, una vez tenemos las componentes principales, encontrar un equilibrio entre la cantidad de datos que se pueden guardar y la información que se está dispuesto perder.\\

Más adelante, vemos una introducción a series temporales, es decir, observaciones de una variable en periodos de tiempo regulares, y algunas formas de modelarlas y propiedades de los modelos utilizados para ello.\\

Por último, vemos el análisis de componentes principales dinámico generalizado, que es una aplicación del análisis de componentes principales a series temporales, buscando reducir la dimensión de un conjunto de series temporales buscando las componentes principales, que serán nuevas series, intentando no perder mucha información, aunque en este caso dichas componentes no tienen por qué ser combinaciones lineales de las series temporales originales. La cantidad de información a perder se podrá elegir, de nuevo, en el número de componentes que se desea usar.

\section{Análisis de Componentes Principales}
Para la realización de esta sección sobre análisis de componentes principales se han tenido en cuenta las referencias \cite{anderson} y \cite{sanchez}.\\

Supongamos que tenemos un vector de variables aleatorias $X =  (X_1, \dots ,$ $X_p)^T$, donde cada variable aleatoria ha sido obtenida midiendo una cierta información. Teniendo en cuenta que se está midiendo información, es usual que haya gran cantidad de variables y que haya una cierta relación entre ellas. Las componentes principales son un conjunto de combinaciones lineales de las variables originales $X$ que a diferencia de estas están incorreladas entre sí, además de poseer otras propiedades estadísticas. El principal objetivo de las componentes principales es reducir la dimensión original, de forma que haya menos datos que guardar, pero intentando perder la menor cantidad de información posible. Esto se consigue buscando un nuevo sistema de ejes coordenados (que será una rotación del original) llamadas componentes principales, que intentarán explicar la estructura de covarianza del vector aleatorio $X$.\\

Vamos a suponer el vector $X$ modelizado por una distribución normal $p$-dimensional de media cero y vamos a construir las componentes principales iterativamente haciendo uso de la técnica de los multiplicadores de Lagrange.\\

\subsection{Definición de las componentes principales}

Supongamos pues de aquí en adelante $X = (X_1, X_2, \dots, X_p)^T$ con matriz de covarianzas $\Sigma$ semidefinida positiva y con $\lambda_1 \geq \lambda_2 \geq \lambda_p \geq 0$ las raíces características correspondientes a $\Sigma$. Sean los vectores $l_i^T = (l_{i1}, l_{i2}, \dots, l_{ip}), i=1,\dots,p$ y sean las combinaciones lineales 
\begin{equation*}
  \left\lbrace
  \begin{array}{l}
     Y_1 = l_1^TX = l_{11}X_1 + \cdots + l_{1p}X_p \\
     \vdots \\
     Y_p = l_p^TX = l_{p1}X_1 + \cdots + l_{pp}X_p \\
  \end{array}
  \right.
\end{equation*}

Consideremos el vector aleatorio $Y=(Y_1, \dots, Y_p)^T$. Para cualesquiera dos componentes de $Y$, $i$ y $j$, tenemos que:
\[	\var{(Y_i)} = \var{(l_i^TX)} = \mathbb{E}[(l_i^TX - \mathbb{E}[l_i^TX])^2]. \]
Ahora, como el vector $l_i$ no depende de $X$ y dado que hemos supuesto que la media de $X$ es cero, nos queda:
\[	\mathbb{E}[(l_i^TX - \mathbb{E}[l_i^TX])^2] = \mathbb{E}[(l_i^TX)^2] = \mathbb{E}[l_i^TXX^Tl_i] = l_i^T \mathbb{E}[XX^T] l_i. \] 

Vamos a desarrollar esta esperanza:
\[	\mathbb{E}[XX^T] = \mathbb{E} \left[ \left( \begin{array}{c}
X_1 \\
\vdots \\
X_p \end{array} \right)
\left( \begin{array}{ccc}
X_1 & \cdots & X_p  \end{array} \right) \right] = 
\mathbb{E} \left[ \left( \begin{array}{cccc}
X_1^2 & X_1X_2 & \cdots & X_1X_p \\
\vdots & \vdots & \vdots & \vdots \\
X_pX_1 & X_pX_2 & \cdots & X_p^2 \end{array} \right) \right]
\]

Ahora, como la esperanza de cada $X_i$ es cero, la esperanza de esa matriz es justo la matriz de covarianzas de $X$, $\Sigma$. Por tanto, la varianza de cada $Y_i$ nos queda $\var{(Y_i)} = l_i^T\Sigma l_i$.\\

Veamos ahora cuál es la covarianza entre dos componentes $Y_i$ e $Y_j$, por un razonamiento semejante:

\[ \cov{(Y_i, Y_j)} = \cov{(l_i^TX, l_j^TX)} = \mathbb{E}[(l_i^TX - \mathbb{E}[l_i^TX])(l_j^TX - \mathbb{E}[l_j^TX])] = \] 
\[ = \mathbb{E}[(l_i^TX)(X^Tl_j)] = l_i \mathbb{E}[XX^T] l_j = l_i \Sigma l_j.	\]

Se llaman \textbf{componentes principales (CP)} a las combinaciones lineales $Y_1, \dots, Y_p$ que son incorreladas entre sí y tales que hacen máximas las varianzas $l_i^T\Sigma l_i,\ \ i=1,\dots,p$ sujetas a ciertas restricciones que pasamos a precisar.

\subsection{Retristricciones sobre las componentes principales en el proceso de construcción}

\begin{enumerate}
\item Consideremos la combinación lineal de varianza máxima, a la que vamos a llamar $Y_1$, de forma que esta varianza será $\var{(Y_i)}=l_i^T \Sigma l_i$. Esta varianza aumentará si multiplicamos el vector $l$ por una constante positiva, de modo que vamos a imponer la restricción $l_i^Tl_i = 1$ para todo $i=1, \dots, p$.
\item La primera componente principal será por tanto la combinación lineal $Y_1 = l_1^TX$ tal que hace máxima la varianza $l_1^T \Sigma l_1$ sujeta a la restricción $l_1^Tl_1 = 1$.
\item Llamamos segunda componente principal a la combinación lineal $Y_2 = l_2^TX$ tal que hace máxima $\var{(Y_2)}$ con la restricción $l_2^Tl_2 = 1$ y con la restricción de ser incorrelada con $Y_1$, es decir, $\cov{(l_1^TX, l_2^TX)} = 0$.
\item Continuamos el proceso hasta construir las $p$ combinaciones lineales $Y_1, \dots, Y_p$ definidas como aquellas que maximizan, para cualquier $i=1, \dots, p$, $\var{(l_i^TX)}$ sujetas a $l_i^Tl_i = 1$ y a $\cov{(l_i^TX, l_k^TX)} = 0$ para todo $k < i$. Cada uno de estos problemas de máximos condicionados se resuelven mediante multiplicadores de Lagrange.
\end{enumerate}

Vamos a ver ahora la resolución de dichos problemas de Lagrange para las dos primeras componentes principales y para una genérica $r$ con $ 1 \leq r \leq p$.

\subsection{Cálculo de la primera componente principal}

Se define la primera componente principal como 
\[	Y_1 = l_1^TX,\ l_1^Tl_1 = 1	\]
sujeta a 
\[	\var{(Y_1)} = l_1^T\Sigma l_1 = \underset{l}{\text{máx}}\ \var{(l^TX)}.	\]

Por lo tanto, tenemos que resolver el siguiente problema de multiplicadores de Lagrange:
\begin{equation*}
  \left\lbrace
  \begin{array}{l}
     \underset{l}{\text{máx}}\ \var{(l^T\Sigma l)} \\
     l^Tl=1 \\
  \end{array}
  \right.
\end{equation*}

Definimos la función de Lagrange $\mathcal{L}_1(l, \lambda) = l^T \Sigma l - \lambda(l^Tl - 1)$
y derivamos a igualamos a cero para resolver.\\

Para derivar $l^Tl$, con $l$ un vector, con respecto a $l$, tenemos que tener en cuenta que 
\[	l^Tl = \sum_{i=1}^p l_i^2	\]
y si hacemos el gradiente, nos queda $2(l_1, l_2, \dots, l_p)^T = 2l$.\\

Para derivar $l^T \Sigma l$ con respecto a $l$ sólo tenemos que tener en cuenta que la matriz $\Sigma$ es simétrica y la proposición sobre derivación de una forma cuadrática en el capítulo de introducción, con lo que nos queda que $\frac{\partial l^T \Sigma l}{\partial l} = 2 \Sigma l$.

Por tanto, si derivamos e igualamos a cero en $\mathcal{L}_1(l, \lambda) = l^T \Sigma l - \lambda(l^Tl - 1)$ obtenemos:

\[	\frac{\partial \mathcal{L}_1(l, \lambda)}{\partial l} = 2 \Sigma l - 2 \lambda l = 0 \Rightarrow (\Sigma - \lambda I)l = 0
	\]

Para poder obtener una solución de $(\Sigma - \lambda I)l = 0$ con $l^Tl=1$ tiene que darse que $l \neq 0$ y por tanto para que el sistema de ecuaciones tenga solución tiene que darse $|\Sigma - \lambda I| = 0$ lo que implica que $\lambda$ es un valor propio de $\Sigma$.\\
Ahora, si multiplicamos $(\Sigma - \lambda I)l = 0$ por $l^T$ a la izquierda, obtenemos
$l^T \Sigma l - \lambda l^Tl = 0 \Rightarrow l^T \Sigma l = \lambda$, es decir, $\lambda$ es la varianza de $Y_1$. Por tanto, para tener máxima varianza tenemos que utilizar $\lambda = \lambda_1$ el mayor valor propio de $\Sigma$. Ahora, $\var{(Y_1)} = l^T \Sigma l = \lambda_1 \Rightarrow \Sigma l = l \lambda_1 = \lambda_1 l$ ya que $l^Tl=1$, y el vector $l$ que cumple $\Sigma l = \lambda_1 l$ es $l = e_1$ el vector propio asociado al valor propio $\lambda_1$.\\

Tenemos por tanto que la primera componente principal es $Y_1$ = $e^T_1X$ con $e_1^Te_1 = 1$, $e_1$ el vector propio asociado al mayor valor propio $\lambda_1$ de $\Sigma$ la matriz de covarianzas del vector $X$.

\subsection{Cálculo de la segunda componente principal}

Tenemos que obtener ahora una segunda combinación lineal $Y_2 = l^TX$ con $l^Tl=1$, incorrelada con $Y_1$ y que maximice la varianza de nuevo. Es decir, tenemos que resolver el problema:
\[	\underset{l}{\text{máx}}\ \var{(l^TX)} \ \ \text{sujeto a} \ \  l^Tl = 1,\ \ \cov{(Y_2, Y_1)} = l^T \Sigma e_1 = 0.	\]

De la restricción $l^T \Sigma e_1 = 0$ y utilizando que $\Sigma e_1 = \lambda_1 e_1$ por ser $e_1$ vector propio de $\Sigma$, nos queda $l^T \lambda_1 e_1 = \lambda_1 l^T e_1 = 0$ y como $\lambda_1 \neq 0$ siempre que $\Sigma \neq 0$ por ser $\lambda_1$ el mayor valor propio de $\Sigma$, nos queda que $l^T e_1 = 0$, es decir, $l$ y $e_1$ son ortogonales.\\

Para resolver el problema definimos la función de Lagrange con dos multiplicadores
\[	\mathcal{L}_2(l, \lambda, v) = l^T \Sigma l - \lambda (l^Tl - 1) - v(l^T \Sigma e_1) 	\]
y derivamos e igualamos a cero para resolver y encontrar el máximo:
\[	\frac{\partial \mathcal{L}_2(l, \lambda, v)}{\partial l} = 2 \Sigma l - 2 \lambda l - v l^T \Sigma e_1 =	\]

Multiplicando por la izquierda por $e_1^T$ y utilizando que $\Sigma e_1 = \lambda_1 e_1$ y que $e_1^T e_1 = 1$ nos queda:
\begin{equation} \label{eq:CP2}
	2 e_1^T \Sigma l - 2 \lambda e_1^T l - v \lambda_1 e_1^T e_1 = 2 e_1^T \Sigma l - 2 \lambda e_1^T l - v \lambda_1 =
\end{equation}

y como $l$ y $e_1$ son ortogonales:
\[	= 2 e_1^T \Sigma l - v \lambda_1 = 2 \cov{(Y_1, Y_2)} - v \lambda_1 = -v \lambda_1	\]
ya que $Y_1$ e $Y_2$ están incorreladas (luego su covarianza es nula), con lo que nos queda que $-v \lambda_1 = 0 \Rightarrow v = 0$.

Entonces, volviendo a la expresión (\ref{eq:CP2}) y sabiendo que $v=0$ obtenemos de nuevo que tiene que cumplirse que $(\Sigma - \lambda I) l = 0 \Rightarrow |\Sigma - \lambda I| = 0$, con lo que $\lambda$ tiene que ser de nuevo un valor propio de $\Sigma$ y $\var{(Y_2)} = \lambda$ por el mismo razonamiento que con la primera componente principal, y si queremos que sea máxima tenemos que elegir $\lambda_2$ el segundo mayor valor propio de $\Sigma$ y como $l$ el vector propio asociado, $e_2$. Por tanto, la segunda componente principal es $Y_2 = e_2^TX$.

\subsection{Cálculo de la $(r+1)$-ésima componente principal con $1 \leq r+1 \leq p$ }

En este caso tenemos 
\[	Y_{r+1} = l^TX\ \ \text{sujeta a}\ \ l^Tl = 1,\  l^T\Sigma e_i = 0, \ i = 1, \dots, r	\]
y se define la función de Lagrange
\[	\mathcal{L}_{r+1}(l, \lambda, \mathbf{v}) = l^T \Sigma l - \lambda (l^T l - 1) - \sum_{i=1}^r v_i l^T \Sigma e_i	\]
con $\mathbf{v} = (v_1, v_2, \dots, v_r)$.\\

\begin{proposicion}
Con la función de Lagrange que acabamos de definir y suponiendo que los valores propios de $\Sigma$, $\lambda_i \neq 0,\  i=1, \dots, r$, entonces $v_i = 0, \  i=1, \dots, r$ y el problema de maximización anterior se reduce a $(\Sigma - \lambda I)l = 0$
\end{proposicion}
\textbf{Demostración}\\

Comenzamos derivando la función de Lagrange con respecto a $l$ e igualando a cero:
\begin{equation}\label{eq:CPr}
	\frac{\partial \mathcal{L}_{r+1}(l, \lambda, \mathbf{v})}{\partial l} = 2 \Sigma l - 2 \lambda l - \sum_{i=1}^r v_i \Sigma e_i = 0.
\end{equation}
Multiplicamos (\ref{eq:CPr}) por $e_j^T$ por la izquierda con $j \in \{1, \dots, r\}$,
\[	2 e_j^T \Sigma l - 2 \lambda e_j^T l - \sum_{i=1}^r v_i e_j^T \Sigma e_i = 0.	\]

Utilizamos ahora que $\Sigma e_i = \lambda_i e_i$ para todo $i = 1, \dots, r$ y que $l$ es ortogonal con $e_j$ por ser $Y_{r+1}$ e $Y_j$ incorreladas, tenemos:
\[ - \sum_{i=1}^r v_i e_j^T \lambda_i e_i = 0.	\]

Por construcción, $e_i$ es ortogonal a $e_j$ excepto cuando $i=j$ y $e_j^T e_j = 1$, con lo cual:
\[	- \sum_{i=1}^r v_i e_j^T \lambda_i e_i = - v_j \lambda_j e_j^T e_j = - v_j \lambda_j = 0.	\]

Con lo que $v_j = 0$ por hipótesis. Como esto lo hemos hecho para $j \in \{1, \dots, r \}$ nos queda que $v_i = 0$ para todo $i$ entre $1$ y $r$, como queríamos demostrar, y si volvemos a la ecuación (\ref{eq:CPr}) y utilizamos esta información obtenemos
\[	2 \Sigma l - 2 \lambda l = 0 \Rightarrow (\Sigma - \lambda I) l = 0. \]

\begin{flushright}
$\blacksquare$
\end{flushright}

Si $\lambda_{r+1} \neq 0$, por un razonamiento análogo a las componentes primera y segunda, este problema se resuelve tomando $\lambda = \lambda_{r+1}$ y $l = e_{r+1}$ y se obtiene la $(r+1)$-ésima componente principal $Y_{r+1} = e^T_{r+1}X,\ \var{(Y_{r+1})} = \lambda_{r+1}$.\\
% TODO ????????
Por otro lado, si $\lambda_{r+1} = 0,\ \lambda_i \neq 0,\ i \neq r+1$, se puede reemplazar $e_{r+1}$ por una combinación lineal de $e_{r+1}$ y los $e_j$ de forma que $\lambda_j \neq 0$ definiendo así un nuevo $e_{r+1}'$ que es ortogonal a todos los $e_j$ con $j = 1, \dots, r$.\\
%??????????

De esta forma construimos una matriz con los vectores propios $E = (e_1, \dots, e_p)$ y otra matriz con los valores propios, diagonal en este caso, $\Lambda = diag(\lambda_1, \dots, \lambda_p)$ con $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p \geq 0$. Como $E^T E = I$ y $\Sigma E = E \Lambda$, tenemos que $E^T \Sigma E = \Lambda$.\\

En el caso de que $\Sigma$ tenga raíces múltiples, tenemos el siguiente teorema:

\begin{teorema}
Si $\lambda_{r+1} = \cdots = \lambda_{r+m} = \lambda$, entonces $\Sigma - \lambda I$ es de rango $p-m$. Los correspondientes vectores característicos $e_{r+1}, \dots, e_{r+m}$ están unívocamente determinados, salvo multiplicación por la derecha por una matriz ortogonal.
\end{teorema}

\textbf{Demostración}\\

Como la $i$-ésima columna de $E$ es $e_r$, vector propio, se da que $(\Sigma - \lambda_i I)e_i = 0$, y por lo tanto tenemos que esto se cumple para todo $i = r+1, \dots, r+m$ y en concreto podemos escribir $(\Sigma - \lambda I)e_i = 0$. Esto implica que $e_{r+1}, \dots, e_{r+m}$ son soluciones de $(\Sigma - \lambda I)e = 0$. Para ver que son linealmente independientes basta darse cuenta de que $\Sigma$ es una matriz real simétrica y por tanto, diagonalizable, por lo que existe una matriz ortogonal $P$ de forma que $\Sigma P = PD$, con $D$ diagonal. Las columnas de $P$ son vectores propios y forman una base, por lo que tienen que ser linealmente independientes. Esto implica que la multiplicidad algebraica y geométrica de $\Sigma$ coinciden, y por tanto si el valor propio $\lambda$ tiene multiplicidad algebraica igual a $m$, entonces hay exactamente $m$ vectores propios linealmente independientes, $e_{r+1}, \dots, e_{r+m}$, y por tanto el rango de la matriz $\Sigma - \lambda I$ es $p-m$.\\ %???????

Ahora, como $E^* = (e_{r+1},\dots, e_{r+m})$ es un conjunto de soluciones de $(\Sigma - \lambda I) e = 0$ linealmente independiente, cualquier otra solución debe ser combinación lineal de los elementos de $E^*$, es decir, se pueden escribir el resto de soluciones como $E^*A$ con $A$ matriz no singular. Sin embargo, todos los elementos de $E^*$ cumplen la condición de ortogonalidad $e_i^T e_i = 1 \Rightarrow E^{*T} E = I$ que aplicadas a las combinaciones lineales nos da $I = (E^*A)^T (E^*A) = A^TE^{*T}E^*A = A^TA$, por lo que $A$ es ortogonal.

\begin{flushright}
$\blacksquare$
\end{flushright}

\newpage

\section{Introducción a las Series Temporales}

Para realizar esta introducción a las series temporales se han seguido los libros \cite{pena05} y \cite{box08}.\\

\begin{definicion}[Serie temporal]
Una serie temporal (o serie cronológica) es un conjunto de observaciones de valores de una variable generadas secuencialmente en el tiempo, normalmente a intervalos regulares: $\{y(t); t=1,2,\dots, N\}$
\end{definicion}

De forma informal, decimos que si la serie temporal oscila alrededor de un valor constante, es estacionaria. En caso contrario, se llamará no estacionaria. La definición estricta es la siguiente:
\begin{definicion}[Serie estacionaria]
Una serie temporal es estacionaria si se cumplen las dos siguientes condiciones:
\begin{enumerate}
\item Las distribuciones marginales de todas las variables son iguales.
\item Las distribuciones finito-dimensionales de cualquier conjunto de variables sólo dependen de los retardos entre ellas.
\end{enumerate}
\end{definicion}
En particular, estas condiciones implican que la media y la varianza son constantes y que la covarianza entre dos variables $y(t)$ e $y(t+k)$ sólo depende de $k$, es decir, las propiedades estadísticas son independientes del instante de tiempo en el que se observan las muestras.\\


Es interesante construir modelos para representar la evolución de una serie temporal y hacer predicciones sobre valores futuros de la misma. Estos modelos se basan en valores pasados y en la hipótesis de que las condiciones futuras serán análogas a las pasadas, de forma que si conocemos los valores pasados se pueden utilizar para hacer una predicción sobre valores futuros. La \textbf{ecuación general para el modelado de una serie temporal} es la siguiente:
\[	y(t) = F(y(t-1), y(t-2), \dots, y(t-k), \dots) + e(t)	\]
donde $e(t)$ es un componente aleatorio que recoge el resto de efectos que actúan sobre la serie, sobre el que se supone que las variables son independientes entre sí y que siguen una distribución normal de media cero y varianza constante.\\
Si existe otro conjunto de variables que puedan explicar el modelo (variables exógenas o explicativas) la ecuación general pasa a ser:
\[	y(t) = F(y(t-1), y(t-2), \dots, x(t), x(t-1), \dots) + e(t)	\]
y el problema pasa a denominase \textit{Modelado de sistemas}.\\

\begin{nota}
De aquí en adelante, se utilizará, para series temporales, la notación $\widehat{y}(t)$ para notar un valor predicho de la serie $\{y(t)\}$.
\end{nota}

El objetivo es por tanto predecir $\widehat{y}(t+h)$. A $h$ se le llama \textbf{horizonte de la predicción}. Hay varias formas de hacerlo:
\begin{enumerate}
\item \textbf{Método directo}: Se busca un modelo exactamente de la forma
\[	y(t+h) = F(y(t-1), y(t-2),\dots) + e(t)	\]
\item \textbf{Método recursivo}: Se aplica el modelo más general
\[	y(t) = F(y(t-1), y(t-2),\dots)+e(t)	\]
de forma recursiva hasta obtener el horizonte que deseamos predecir
\[	\widehat{y}(t+1) = F(y(t), y(t-1), y(t-2),\dots)	\]
\[	\widehat{y}(t+2) = F(\widehat{y}(t+1),y(t),y(t-1),\dots)	\]
\[ \vdots	\]
\end{enumerate}

Además hay muchos tipos de modelos, que se pueden clasificar según la función $F$ sea lineal: modelos AR, MA, ARMA, ARIMA, SARIMA, ARX, etc., o no lineal: funciones de base radial (RBF), perceptrones multicapa (MLP), sistemas difusos, máquinas de vectores soporte (SVM), etc.\\

Llegados a este punto, conviene preguntarse cuántas y qué variables conviene utilizar en la predicción, ya que puede que no todas influyan lo mismo en los valores futuros y utilizarlas todas no es computacionalmente eficiente. Además, habrá que elegir también qué modelo utilizar para la predicción. Para contestas a estas preguntas es para lo que se utilizan la función de autocorrelación, autocorrelación parcial o autocovarianzas vistas en el capítulo de introducción, como vamos a ver para algunos modelos lineales a continuación.\\

\subsection{Proceso de ruido blanco}

Dado un conjunto infinito de muestras $\{e(t);\ t=1,2,\dots\}$, decimos que $\{e(t)\}$ es una señal de ruido blanco si es una secuencia de variables incorreladas de media cero y varianza constante, es decir, si
\begin{equation*}
	r_e(k) = 
  \left\lbrace
  \begin{array}{l}
     1 \ \ \ \ k = 0 \\
     0 \ \ \ \ k \neq 0 \\
  \end{array}
  \right.
\end{equation*}
con $r_e(k)$ la autocorrelación de orden $k$. Esto es así porque $\cov{(e(t),e(t+k))} = 0$ siempre que $k \neq 0$, es decir, las muestras están incorreladas para todos los desfases temporales y por tanto conocer los valores pasados no da información sobre los valores futuros.\\
Si cogemos una muestra finita de $N$ valores de una señal de ruido blanco obtenemos (para $k \neq 0$) que
\[	r_e(k) \sim \mathcal{N} \left( 0, \frac{1}{N} \right), \ \ \ \ \phi_e(k) \sim \mathcal{N} \left( 0, \frac{1}{N} \right)	\]

es decir, tanto la función de autocorrelación como la función de autocorrelación parcial siguen una normal de media cero y varianza $1/N$. Esto nos sirve para identificar una señal de ruido blanco, que no sería posible sólo viendo representada la serie temporal. De esta forma lo que tenemos que hacer es calcular las funciones de autocorrelación y autocorrelación parcial y utilizar la propiedad de la distribución normal de que al obtener una muestra de esta distribución hay una probabilidad de $0,9544$ de que pertenezca al intervalo $[\mu - 2 \sigma, \mu + 2 \sigma]$ con $\mu$ notando la media y $\sigma$ la desviación típica. Por tanto, basta con comprobar si el $95\%$ de dichas funciones en la muestra que disponemos de la serie temporal se queda en el intervalo $[- \frac{2}{\sqrt{N}}, \frac{2}{\sqrt{N}}]$.\\

Es importante conocer y saber identificar una señal de ruido blanco para no intentar predecir valores futuros sobre ellas con los valores pasados.

\subsection{Modelos ARMA}

Dada una serie temporal $\{y(t);\ t=1,\dots,N\}$ a la que suponemos con media cero, podemos modelar dicha serie mediante una relación lineal del tipo
\[	y(t) = - \sum_{k=1}^{\infty} a_k y(t-k) + e(t), \]
con $a_i$ constantes, o, equivalentemente, con la notación introducida en la sección 4 del primer capítulo,
\[	A_{\infty}(q)y(t) = e(t), \ \ \ \ A_{\infty}(q) \equiv 1 + \sum_{k=1}^{\infty} a_k q^{-k}.	\]

Si despejamos de esta expresión $y(t)$ nos queda:
\[	y(t) = \frac{1}{A_{\infty}(q)}e(t) \equiv H_{\infty}(q)e(t) = \sum_{k=0}^{\infty} h_k e(t-k) = h(t) \ast e(t),	\]

que es la expresión de un filtro digital lineal al que le estamos metiendo como entrada un ruido blanco.\\

Con un número infinito de parámetros, los modelos $A_{\infty}$ y $H_{\infty}$ son equivalentes para modelar una serie temporal. Sin embargo, en la práctica, no se pueden utilizar infinitos parámetros.\\
Si cortamos los parámetros en $n$, dichos modelos dejan de ser equivalentes y según utilicemos la expresión $A_n$ o $H_n$ obtenemos los modelos auto-regresivos (AR) o los de medias móviles (MA), respectivamente.

\subsubsection{Modelos Auto-Regresivos (AR)}

Su expresión general, con $n_a$ parámetros, como ya hemos visto, es:
\[	A(q)y(t) = e(t) \ \ \text{con} \ \ A(q) \equiv 1 + \sum_{k=1}^{n_a} a_k q^{-k},	\]
luego,
\[ y(t) + \sum_{k=1}^{n_a} a_k y(t-k) = e(t) \Rightarrow y(t) = - \sum_{k=1}^{n_a} a_k y(t-k) + e(t)	\]
con $e(t)$ una señal de ruido blanco.\\

También podemos hacer:
\[	y(t) = H(q) e(t) = \frac{1}{A(q)}e(t) = \sum_{k=0}^{\infty} h_k e(t-k),	\]
obteniendo así este modelo como un filtro digital y entonces dicho filtro será estable si cumple 
\begin{equation} \label{eq:Festable}
\sum_{k=0}^{\infty} |h_k| < \infty.
\end{equation}

Otra condición para que sea estable es que las raíces de $A(q) = 0$ se queden dentro del círculo unidad.\\

Además, tenemos que se cumple la propiedad de que si $e(t)$ es una señal de ruido blanco y el filtro lineal generador de la serie es estable, entonces la serie es estacionaria. Por tanto, si se cumple (\ref{eq:Festable}) la serie generada por un proceso auto-regresivo es siempre estacionaria.\\

Vamos a obtener ahora las expresiones de la función de autocorrelación y de autocorrelación parcial, que nos servirán bien para reconocer una serie generada con este modelo, bien para saber si podemos modelar una serie que ya tenemos con este modelo. Multiplicando la expresión general
\[	y(t) + a_1y(t-1) + a_2 y(t-2) + \cdots a_{n_a} y(t - n_a) = 0 \]
por $y(t-k)$ con $k>0$, tomando esperanzas y teniendo en cuenta que las $a_i$ son constantes y la serie temporal $\{y(t)\}$ la hemos supuesto de media cero, nos queda:
\[	\mathbb{E}[y(t)y(t-k)] + \mathbb{E}[a_1y(t-1)y(t-k)] + \cdots + \mathbb{E}[a_{n_a}y(t-n_a) y(t-k)] = 0 \Rightarrow	\]
\[	\Rightarrow c_y(k) + a_1 c_y(k-1) + \cdots + a_{n_a} c_y(k-n_a) = 0,	\]
con $c_y(k)$ la autocovarianza de orden $k$, y si ahora dividimos por $c_y(0) = \sigma^2$ la varianza de $y$ tenemos que los coeficientes de autocorrelación verifican:
\[	r_y(k) + a_1r_y(k-1) + \cdots + a_{n_a} r_y(k-n_a) = 0.	\]

Por otra parte, por la propia definición de autocorrelación parcial, si tomamos $k>n_a$ tenemos que $\phi_y(k) = 0$.\\

Si descomponemos el polinomio $A(q)$ en función de sus raíces tendremos que, en general, $r_y(k)$ será una suma de funciones exponenciales y sinusoidales con amplitudes decrecientes exponencialmente. Tanto esto como tener en cuenta que la autocorrelación parcial se anula para todo $k>n_a$ nos hace más fácil reconocer una serie temporal que ha sido generada con un modelo auto-regresivo sin más que mirar las gráficas de las representaciones de las funciones de autocorrelación y de autocorrelación parcial.\\

Este modelo que hemos visto es el auto-regresivo general de orden $n_a$, AR($n_a$). Podemos particularizar este modelo a un $n_a$ concreto. Por ejemplo, la expresión del modelo AR(2) sería
\[	(1+a_1q^{-1}+a_2q^{-2})y(t) = e(t) \Rightarrow y(t) = -a_1y(t-1)-a_2y(t-2) + e(t).	\]

\subsubsection{Modelos de medias móviles (MA)}

En este caso, los nuevos valores de la serie temporal dependen de valores antiguos de $e(t)$, la señal de ruido blanco. La expresión general de un modelo MA de orden $n_c$ es
\[ y(t) = C(q)e(t)\ \ \text{con}\ \ C(q) = 1 + \sum_{k=1}^{n_c} c_k q^{-k} \Rightarrow	\]
\begin{equation} \label{eq:MA}
y(t) = e(t) + c_1e(t-1) + c_2 e(t-2) + \cdots c_{n_c} e(t-n_c).
\end{equation}

Ahora, la serie resultante será siempre estacionaria por ser suma de procesos estacionarios, ya que los procesos de ruido blanco lo son por definición.\\

En el caso de medias móviles, tenemos que se cumplen las dos siguientes propiedades:
\begin{enumerate}
\item Los coeficientes de la función de autocorrelación son cero para todo $k>n_c$.
\item La función de autocorrelación parcial es aproximadamente una suma de funciones exponenciales y senoidales con amplitudes decrecientes exponencialmente.
\end{enumerate}

La primera propiedad es fácil de comprobar multiplicando la expresión (\ref{eq:MA}) por $y(t-k)$ con $k>n_c$, tomando esperanzas y teniendo en cuenta que habíamos supuesto $\{y(t)\}$ de media cero, de forma que obtenemos primero las covarianzas:
\begin{equation} \label{eq:MAcov}
	c_y(k) = \mathbb{E}[(y(t)y(t-k))] -  \mathbb{E}[(e(t)y(t-k))] - \cdots - \mathbb{E}[c_{n_c} e(t-n_c) y(t-k).]
\end{equation}

Ahora, según la expresión (\ref{eq:MA}), $y(t-k)$ depende de $e(t-k)$ y de los $n_c$ antiguos valores de $e(t-k)$, pero no depende de los valores $e(t-k+1)$ en adelante y como $k>n_c$ todas las esperanzas en la ecuación (\ref{eq:MAcov}) son cero y por tanto $c_y(k) = 0 \Rightarrow r_y(k) = 0$ siempre que $k>n_c$.\\

El cálculo para comprobar la segunda propiedad es más complicado y no se expone aquí.\\

Estas dos propiedades nos dan, como en el caso del modelo auto-regresivo, una forma sencilla de reconocer una serie que ha sigo generada por un modelo de medias móviles puro viendo una representación de sus funciones de autocorrelación y autocorrelación parcial.

\subsubsection{Modelo ARMA}

Ya hemos comentado que cuando utilizamos un número infinito de parámetros los modelos auto-regresivos y los de medias móviles son equivalentes. Cuando volvemos a juntar estos dos modelos utilizando $n_a$ parámetros para el auto-regresivo y $n_c$ parámetros para el de medias móviles, obtenemos el modelo ARMA de orden $(n_a,n_c)$, ARMA($n_a,n_c$), cuya expresión general es:
\[	A(q)y(t) = C(q)e(t),\ \text{con}\ A(q) = 1 + \sum_{k=1}^{n_a} a_kq^{-k},\ C(q) = 1 + \sum_{k=1}^{n_c} c_k q^{-k} \Rightarrow	\]

\begin{equation}
y(t) = - \sum_{k=1}^{n_a} a_k y(t-k) + e(t) + \sum_{k=1}^{n_c} c_k e(t-k).
\end{equation}

Igual que con los modelos anteriores, podemos tratarlo como un filtro digital $H$
\[	y(t) = H(q)e(t) = \frac{C(q)}{A(q)}e(t),	\]
con $e(t)$ una señal de ruido blanco. El filtro será estable (y por tanto la serie generada estacionaria) si las raíces de $A(q) = 0$ se quedan en el círculo unidad, que es la misma condición que con los modelos auto-regresivos, ya que para los modelos de medias móviles siempre se daba que el filtro fuera estable.\\

En este caso es más difícil reconocer una serie temporal generada con un modelo ARMA, ya que se puede comprobar que tanto la función de autocorrelación como la función de autocorrelación parcial son exponenciales y/o senoidales con amplitudes decrecientes de forma exponencial, en ambos casos decreciendo a partir de $n_c-n_a$.

\subsubsection{Preprocesamiento de series temporales}

Para modelar una serie temporal con uno de los modelos vistos es necesario que la preprocesemos hasta que tenga media cero y sea estacionaria, ya que es así como generan series estos modelos. Para que tengan media cero basta con calcular la media de la serie y restársela. Para hacerla estacionaria, normalmente se diferencia el número de veces que sea necesario si la serie presenta tendencia
\[	y_d(t) = y(t) - y(t-1) = (1 - q^{-1})y(t),	\]
o si la serie tiene tendencia periódica
\[	y_d(t) = y(t) - y(t-s) = (1 - q^{-s})y(t).	\]

\newpage

\section{Análisis de Componentes Principales Dinámicas Generalizado}

El análisis de componentes principales dinámicas generalizado es una aplicación del análisis de componentes principales a las series temporales, de forma que lo que se intenta es reducir la dimensión de varias series temporales observadas, haciendo que haya que almacenar menos datos, pero intentando perder el mínimo de información posible. Ha sido propuesto por Daniel Peña y Victor J. Yohai ~\cite{pena16}, aunque todavía no se ha publicado. La reducción de dimensión en series temporales es importante porque el número de parámetros en un modelo crece muy rápido a medida que aumenta el número de series que se observan.

\subsubsection{Antecedentes: El método de componentes principales dinámicas de Brillinger}

Antes del método propuesto por Peña y Yohai, se han propuesto otros métodos para reducir dimensión en series temporales y para predicción y reconstrucción de las mismas. Entre ellos cabe destacar el método de Brillinger, por ser uno de los que más está basado en el análisis de componentes principales básico, y por ser el que toma de base el procedimiento de Peña.\\

En el procedimiento de Brillinger, las componentes principales son combinaciones lineales de las series que han sido observadas y sobre las que se quiere hacer la reducción. Las componentes principales son obtenidas a través de transformadas inversas de Fourier. Además, las series observadas han de ser estacionarias y tener tendencia estacional (o estar preprocesadas para que cumplan estas características). En caso de utilizar series no estacionarias, no está garantizado que la reconstrucción usando las componentes principales obtenidas vaya a ser óptima, es decir, que vaya a tener un error mínimo. Por otro lado, Brillinger dio resultados de consistencia para su procedimiento.\\

A diferencia del método de Brillinger, en el de Peña las componentes principales no son necesariamente combinaciones lineales de las series observadas, se pueden utilizar varias funciones de pérdida y no es necesario que las series observadas sean estacionarias ni estacionales. Esto último lleva a una mejor adaptación a un mayor número de casos en los que las series observadas no tienen estas características, sin necesidad de preprocesarlas.\\

Peña y Yohai dan un método iterativo para obtener las componentes principales dinámicas basándose en un criterio de error cuadrático medio (MSE por sus siglas en inglés, mean squared error) mínimo para la reconstrucción, aunque como se ha comentado, se podría utilizar otro criterio. La reconstrucción de las series se puede hacer en base a $k$ valores anteriores de las componentes principales obtenidas (a los que llamaremos \textbf{lags}) o, equivalentemente, en base a $k$ valores posteriores de las componentes (a los que llamaremos \textbf{leads}). Veamos que en efecto, es equivalente. Supongamos que observamos $m$ series de $T$ valores cada una, $z_{j,t}$ con $1 \leq j \leq m,\ 1 \leq t \leq T$. Si llamamos a $\mathbf{f} = (f_1, f_2, \dots, f_{T+k})$ la primera componente principal dinámica, la reconstrucción de las series originales con $k$ leads puede definirse como una combinación lineal de $\mathbf{f}$, una cierta matriz de coeficientes $\beta = (\beta_{j,i})_{1 \leq j \leq m, 1 \leq i \leq k+1}$ y un vector $\alpha = (\alpha_1, \alpha_2, \dots, \alpha_m)$:
\begin{equation}\label{eq:reconsLeads}
	\widehat{z}_{j,t} = \sum_{i=0}^k \beta_{j,i+1}f_{t+i} + \alpha_j.
\end{equation}
De forma que para reconstruir cada valor $t$ de una serie concreta $j$ estamos utilizando el valor $t$ y los $k$ valores siguientes, en total $k+1$ valores de $\mathbf{f}$.\\

Ahora, si definimos $f_t^* = f_{t+k}$ con $1-k \leq t \leq T$ y $\beta_{j,i}^* = \beta_{j, k+2-i}$ con $1 \leq i \leq k+1$ podemos definir la reconstrucción de las series con $k$ lags como:
\begin{equation}\label{eq:reconsLags}
	\widehat{z}_{j,t} = \sum_{i=0}^k \beta_{j,i+1}^*f_{t-i}^* + \alpha_j.
\end{equation}

De forma que si partimos de la expresión (\ref{eq:reconsLags}) y utilizamos las definiciones de $\beta^*$ y $\mathbf{f}^*$, tenemos:
\[	\widehat{z}_{j,t} = \sum_{i=0}^k \beta_{j,i+1}^*f_{t-i}^* + \alpha_j = \sum_{i=0}^k \beta_{j,k-i+1}f_{t-i+k} + \alpha_j	 = \]
\[ = \beta_{j,k+1}f_{t+k} + \beta_{j,k}f_{t+k-i} + \cdots + \beta_{j,1}f_t + \alpha_j = \sum_{i=0}^k \beta_{j,i+1}f_{t+i} + \alpha_j,	\]
con lo que tenemos la expresión (\ref{eq:reconsLeads}), por lo que son equivalentes.\\

De esta forma, el error cuadrático medio con $k$ leads se define como:
\begin{equation}\label{MSE}
\text{MSE}(\mathbf{f}, \beta, \alpha) = \frac{1}{Tm} \sum_{j=1}^m \sum_{t=1}^T (z_{j,t} - \sum_{i=1}^k \beta_{j,i+1}f_{t+i} - \alpha_j)^2,
\end{equation}

y los valores óptimos para $\mathbf{f},\ \beta$ y $\alpha$ se obtendrán donde el MSE alcance el mínimo.\\

En caso de que se disponga de más de una componente principal dinámica, supongamos $p$ componentes, entonces la reconstrucción de cada una de ellas sería, con $1 \leq r \leq p$,
\[	\widehat{z}_{j,t}^r = \sum_{i=0}^k \beta_{j,i+1}^rf_{t+i}^r + \alpha_j^r,	\]
y la reconstrucción de las series originales pasaría a ser:
\[	\widehat{z}_{j,t} = \sum_{r=1}^p \widehat{z}_{j,t}^r.	\]


\subsection{Algoritmo iterativo para las componentes principales dinámicas}

Vamos a dar ahora, siguiendo los pasos de Peña en \cite{pena16}, un algoritmo iterativo para encontrar los mejores valores de $\mathbf{f}$, $\beta$ y $\alpha$.\\


Sea $\mathbf{C}_j(\alpha_j) = (c_{j,t,q}(\alpha_j))_{1 \leq t \leq T+k,\ 1 \leq q \leq k+1}$ la matriz $(T+k)\times (k+1)$ definida por 
\begin{equation*}
  c_{j,t,q}(\alpha_j) = 
  \left\lbrace
  \begin{array}{l}
     (z_{j,t-q+1} - \alpha_j)\ \ \ \text{si} \ \ \ \text{máx}(1, t-T+1) \leq q \leq \text{mín}(k+1,t) \\
     \ \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ \ \ \ \ \ \text{en otro caso}, \\
  \end{array}
  \right.
\end{equation*}

y sea $\mathbf{D}_j(\beta_j) = (d_{j,t,q,}(\beta_j))$ la matriz $(T+k) \times (T+k)$ dada por
\begin{equation*}
  d_{j,t,q}(\beta_j) = 
  \left\lbrace
  \begin{array}{l}
     \sum_{v=\text{máx}(t-k,1)}^{\text{mín}(t,T)} \beta_{j,q-v+1} \beta_{j,t-v+1} \ \ \ \text{si} \ \ \ \text{máx}(1, t-k) \leq q \leq \text{min}(t+k,T+k) \\
     \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 0 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{en otro caso}. \\
  \end{array}
  \right.
\end{equation*}

Podemos definir 
\[	\mathbf{D}(\beta) = \sum_{j=1}^m \mathbf{D}_j(\beta_j).	\]

Derivando la expresión (\ref{MSE}) con respecto a $f_t$, demostración que se puede encontrar en \cite{penaMas} obtenemos que la primera componente principal dinámica es:
\begin{equation}\label{eq:deff}
	\mathbf{f} = \mathbf{D}(\beta)^{-1} \sum_{j=1}^m \mathbf{C}_j(\alpha)\beta_j.
\end{equation}

Si llamamos $\mathbf{F}(\mathbf{f})$ a la matriz $T \times (k+2)$ cuya fila $t$-ésima es $(f_t, f_{t+1}, \dots,$ $ f_{t+k},1)$ y $\mathbf{z}^{(j)} = (z_{j,1}, \dots, z_{j,T})^T$ la serie temporal observada $j$-ésima, entonces los coeficientes $\beta_j$ y $\alpha_j$ con $1 \leq j \leq m$ se pueden obtener, teniendo en cuenta que estamos siguiendo el método de reconstrucción de mínimo error cuadrático medio, por el estimador de mínimos cuadrados, que es, por definición,
\begin{equation}\label{eq:estimator}
	\left( \begin{array}{c}
	\beta_j \\
	\alpha_j  \end{array} \right) = 
	(\mathbf{F}(\mathbf{f})^T \mathbf{F}(\mathbf{f}))^T \mathbf{F}(\mathbf{f})^T \mathbf{z}^{(j)}.   
\end{equation}

Basándonos en las expresiones (\ref{eq:deff}) y (\ref{eq:estimator}) se puede dar una regla para calcular de forma natural $\beta^{(h)}$, $\alpha^{(h)}$ y $\mathbf{f}^{(h+1)}$ una vez se conoce $\mathbf{f}^{(h)}$ y $\mathbf{f}^{(0)}$. En concreto, definimos $\beta_j^{(j)}$ y $\alpha_j^{(h)}$ con $1 \leq j \leq m$ como
\[	\left( \begin{array}{c}
	\beta_j^{(h)} \\
	\alpha_j^{(h)}  \end{array} \right) = 
	(\mathbf{F}(\mathbf{f}^{(h)})^T \mathbf{F}(\mathbf{f}^{(h)}))^T \mathbf{F}(\mathbf{f}^{(h)})^T \mathbf{z}^{(j)},
\]

y definimos $\mathbf{f}^{(h+1)}$ por
\[	\mathbf{f}^{(h+1)} = \sqrt{T+k} \frac{\mathbf{f}^* - \overline{\mathbf{f}}^*}{||\mathbf{f}^* - \overline{\mathbf{f}}^*||},	\]
donde
\[	\mathbf{f}^* = \mathbf{D}(\beta^{(h)})^{-1} \mathbf{C}(\alpha^{(h)})\beta^{(h)}.	\]

$\mathbf{f}^{(0)}$ se elige como la primera componente principal (no dinámica), completada con $k$ ceros.\\

Las iteraciones del algoritmo se paran cuando, dado un $\varepsilon$ prefijado,
\[	\frac{\text{MSE}(\mathbf{f}^{(h)},\beta^{(h)},\alpha^{(h)}) - \text{MSE}(\mathbf{f}^{(h+1)},\beta^{(h+1)},\alpha^{(h+1)})}{\text{MSE}(\mathbf{f}^{(h)},\beta^{(h)},\alpha^{(h)})} < \varepsilon.	\]







%\end{document}